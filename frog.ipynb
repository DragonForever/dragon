{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rou Zhang 7050809338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/zhangrou/Desktop/courses/INF552/HW4/Frogs_MFCCs.csv'\n",
    "f=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_score={'Bufonidae':1,'Dendrobatidae':2,'Hylidae':3,'Leptodactylidae':4}\n",
    "genus_score={'Adenomera':1,'Ameerega':2,'Dendropsophus':3,'Hypsiboas':4,'Leptodactylus':5,'Osteocephalus':6,'Rhinella':7,'Scinax':8}\n",
    "species_score={\n",
    "'AdenomeraAndre':1, \n",
    "'AdenomeraHylaedactylus':2,\n",
    "'Ameeregatrivittata':3, \n",
    "'HylaMinuta':4, \n",
    "'HypsiboasCinerascens':5,\n",
    "'HypsiboasCordobae':6,\n",
    "'LeptodactylusFuscus':7,\n",
    "'OsteocephalusOophagus':8, \n",
    "'Rhinellagranulosa':9, \n",
    "'ScinaxRuber':10\n",
    "}\n",
    "f['Family']=f['Family'].map(family_score)\n",
    "f['Genus']=f['Genus'].map(genus_score)\n",
    "f['Species']=f['Species'].map(species_score)\n",
    "train_df, test_df = train_test_split(f, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)i. Research exact match and hamming score/ loss methods for evaluating multi- label classification and use them in evaluating the classifiers in this problem.\n",
    "\n",
    "\n",
    "Exact match: Exact match is most strict metric, indicating the percentage of samples that have all their labels classified correctly.\n",
    "\n",
    "\n",
    "Hamming score: The Hamming loss is the fraction of labels that are incorrectly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma': 1.5}\n",
      "0.006947660954145438\n",
      "{'C': 10, 'gamma': 1.5}\n",
      "0.010189902732746642\n",
      "{'C': 10, 'gamma': 1.5}\n",
      "0.009726725335803613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np \n",
    "from sklearn.svm import SVC\n",
    "parameters={'C':[0.01,0.1,1,10],'gamma':[0.1,0.5,1.0,1.5]}\n",
    "svc = SVC(decision_function_shape='ovr')\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "x=train_df.iloc[:,:22]\n",
    "y_family=train_df['Family']\n",
    "y_genus=train_df['Genus']\n",
    "y_species=train_df['Species']\n",
    "\n",
    "clf.fit(x,y_family)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = SVC(C=params['C'],gamma=params['gamma'])\n",
    "best_svc.fit(x,y_family)\n",
    "family_score=best_svc.score(test_df.iloc[:,:22],test_df['Family'])\n",
    "y_pred=best_svc.predict(test_df.iloc[:,:22])\n",
    "hamming=hamming_loss(test_df['Family'], y_pred)\n",
    "print hamming\n",
    "#print family_score\n",
    "\n",
    "\n",
    "clf.fit(x,y_genus)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = SVC(C=params['C'],gamma=params['gamma'])\n",
    "best_svc.fit(x,y_genus)\n",
    "genus_score=best_svc.score(test_df.iloc[:,:22],test_df['Genus'])\n",
    "y_pred=best_svc.predict(test_df.iloc[:,:22])\n",
    "hamming=hamming_loss(test_df['Genus'], y_pred)\n",
    "print hamming\n",
    "#print genus_score\n",
    "\n",
    "clf.fit(x,y_species)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = SVC(C=params['C'],gamma=params['gamma'])\n",
    "best_svc.fit(x,y_species)\n",
    "species_score=best_svc.score(test_df.iloc[:,:22],test_df['Species'])\n",
    "y_pred=best_svc.predict(test_df.iloc[:,:22])\n",
    "hamming=hamming_loss(test_df['Species'], y_pred)\n",
    "print hamming\n",
    "#print species_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. You are welcome to try to solve the problem with both normalized and raw attributes and report the results.\n",
    "\n",
    "\n",
    "In this step, I set parameters={'C':[0.01,0.1,1,10],'gamma':[0.1,0.5,1.0,1.5]} and used GridSearchCV to choose best parameters. \n",
    "\n",
    "For the Family lable, the best C is 10 and the best gamma is 1.5, the  hamming loss is  0.006947660954145438 \n",
    "\n",
    "For the Genus lable, the best C is 10 and the best gamma is 1.5,the hamming loss is 0.010189902732746642\n",
    "\n",
    "For the Species lable, the best C is 10 and the best gamma is 1.5,the hamming loss is \n",
    "0.009726725335803613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "norm_data=pd.DataFrame()\n",
    "for i in range(22):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() \n",
    "    data=pd.DataFrame(min_max_scaler.fit_transform(f.iloc[:,i].reshape(-1,1)))  \n",
    "    norm_data=pd.concat([norm_data,data],axis=1)  \n",
    "norm_data=pd.concat([norm_data,f.iloc[:,22:26]],axis=1)\n",
    "norm_data.columns=f.columns\n",
    "train_df, test_df = train_test_split(norm_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.063918480778138\n",
      "{'C': 10}\n",
      "0.05743399722093567\n",
      "{'C': 10}\n",
      "0.044001852709587785\n"
     ]
    }
   ],
   "source": [
    "x=train_df.iloc[:,:22]\n",
    "y_family=train_df['Family']\n",
    "y_genus=train_df['Genus']\n",
    "y_species=train_df['Species']\n",
    "\n",
    "parameters={'C':[0.01,0.1,1,10]}\n",
    "svc = LinearSVC(penalty='l1',dual=False)\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "clf.fit(x,y_family)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_family)\n",
    "family_score=best_svc.score(test_df.iloc[:,:22],test_df['Family'])\n",
    "print 1-family_score\n",
    "\n",
    "clf.fit(x,y_genus)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_genus)\n",
    "genus_score=best_svc.score(test_df.iloc[:,:22],test_df['Genus'])\n",
    "print 1-genus_score\n",
    "\n",
    "clf.fit(x,y_species)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_species)\n",
    "species_score=best_svc.score(test_df.iloc[:,:22],test_df['Species'])\n",
    "print 1-species_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)iii. Repeat 2(b)ii with L1-penalized SVMs. Remember to normalize the at- tributes.\n",
    "\n",
    "In this step, I set parameters={'C':[0.01,0.1,1,10],'gamma':[0.1,0.5,1.0,1.5]} and used GridSearchCV to choose best parameters. \n",
    "\n",
    "For the Family lable, the best C is 10,the  hamming loss  is 0.063918480778138\n",
    "\n",
    "For the Genus lable, the best C is 10 ,the  hamming loss  is 0.05743399722093567\n",
    "\n",
    "For the Species lable, the best C is 10,the  hamming loss  is 0.044001852709587785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.08012968967114409\n",
      "{'C': 10}\n",
      "0.058360352014821704\n",
      "{'C': 10}\n",
      "0.050023158869847206\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np \n",
    "from sklearn.svm import SVC\n",
    "sm = SMOTE(ratio=\"minority\",random_state=42,kind='svm')\n",
    "parameters={'C':[0.01,0.1,1,10]}\n",
    "svc = LinearSVC(penalty='l1',dual=False)\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "x=train_df.iloc[:,:22]\n",
    "y_family=train_df['Family']\n",
    "y_genus=train_df['Genus']\n",
    "y_species=train_df['Species']\n",
    "\n",
    "x, y_family = sm.fit_sample(x, y_family)\n",
    "x=pd.DataFrame(x)\n",
    "y_family=pd.DataFrame(y_family)\n",
    "clf.fit(x,y_family)\n",
    "params=clf.best_params_\n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_family)\n",
    "family_score=best_svc.score(test_df.iloc[:,:22],test_df['Family'])\n",
    "print 1-family_score\n",
    "x=train_df.iloc[:,:22]\n",
    "x, y_genus = sm.fit_sample(x, y_genus)\n",
    "x=pd.DataFrame(x)\n",
    "y_genus=pd.DataFrame(y_genus)\n",
    "clf.fit(x,y_genus)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_genus)\n",
    "genus_score=best_svc.score(test_df.iloc[:,:22],test_df['Genus'])\n",
    "print 1-genus_score\n",
    "x=train_df.iloc[:,:22]\n",
    "x, y_species = sm.fit_sample(x, y_species)\n",
    "x=pd.DataFrame(x)\n",
    "y_species=pd.DataFrame(y_species)\n",
    "clf.fit(x,y_species)\n",
    "params=clf.best_params_ \n",
    "print params\n",
    "best_svc = LinearSVC(C=params['C'],penalty='l1',dual=False)\n",
    "best_svc.fit(x,y_species)\n",
    "species_score=best_svc.score(test_df.iloc[:,:22],test_df['Species'])\n",
    "print 1-species_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)iv. Repeat 2(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained.\n",
    "\n",
    "\n",
    "In this step, I firstly used \" SMOTE(ratio=\"minority\")\" function to compensate imbalanced classes. And this function will increase the number of minority to the same number of majority  class.\n",
    "\n",
    "Then I set parameters={'C':[0.01,0.1,1,10],'gamma':[0.1,0.5,1.0,1.5]} and used GridSearchCV to choose best parameters. \n",
    "\n",
    "For the Family lable, the best C is 10,the  hamming loss  is 0.08012968967114409\n",
    "\n",
    "For the Genus lable, the best C is 10 ,the  hamming loss  is 0.058360352014821704\n",
    "\n",
    "For the Species lable, the best C is 10,the hamming loss  is 0.050023158869847206\n",
    "\n",
    "\n",
    "We can see the result with using SMOTE in this step actually is very similar to the result we got in last step, so I think imbalanced classes doen't have a heavy influence on the model training. Maybe because the model only depends on  support vector machine ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
