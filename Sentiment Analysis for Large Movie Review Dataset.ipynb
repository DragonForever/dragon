{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read data and do the preprocessing part (including remove stop words and punctuation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:30: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "### First step : loding data and turn it in to a csv form\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords=nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "test_data=[]\n",
    "test_label=[]\n",
    "data_path=\"./\"\n",
    "pos_tag = ['MD','CC','CD','DT','EX','IN','POS','PDT','PRP','RP','WDT','WP','PRP$']\n",
    "\n",
    "def pre_process(line):\n",
    "    #print \"before,\",line\n",
    "    remove_punctuation = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    remove_tag = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    line=remove_punctuation.sub(\" \",line.lower())\n",
    "    line=remove_tag.sub(\" \",line)\n",
    "    line=[w for w in line.split() if w not in stop]\n",
    "    line=\" \".join(line)\n",
    "    #print \"after,\",line\n",
    "    return line \n",
    "##load train data\n",
    "label=[\"pos\",\"neg\"]\n",
    "for i in range(len(label)):\n",
    "    for filename in os.listdir(data_path+\"/train/\"+label[i]):\n",
    "        data = open(data_path+\"/train/\"+label[i]+\"/\"+filename, 'r' ).read()\n",
    "        train_data.append(pre_process(data))\n",
    "        train_label.append(label[i])\n",
    "##load test data\n",
    "for i in range(len(label)):\n",
    "    for filename in os.listdir(data_path+\"/test/\"+label[i]):\n",
    "        data = open(data_path+\"/test/\"+label[i]+\"/\"+filename, 'r' ).read()\n",
    "        test_data.append(pre_process(data))\n",
    "        test_label.append(label[i])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = {'review':train_data,'label':train_label}\n",
    "train = pd.DataFrame(a)\n",
    "b={'review':test_data,'label':test_label}\n",
    "test = pd.DataFrame(b)\n",
    "train.to_csv(\"train.csv\")\n",
    "test.to_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part is machine learning model . Use TF-IDF vectorization of word and then use Chi2 to choose top 130 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_selection import SelectKBest  \n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "from sklearn.feature_selection import mutual_info_classif \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "\n",
    "import io\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "def vectorize(data):    \n",
    "    tfidf = TFIDF()\n",
    "    fit_t = tfidf.fit_transform(data[\"review\"])\n",
    "    weight = pd.DataFrame(fit_t.toarray())\n",
    "    \n",
    "    word=tfidf.get_feature_names()\n",
    "\n",
    "    return word,weight,data[\"label\"].values\n",
    "\n",
    "def vectorize2(data,feature):\n",
    "    print data[:5]\n",
    "\n",
    "    tfidf = TFIDF(vocabulary=feature)\n",
    "    fit_t = tfidf.fit_transform(data[\"review\"])\n",
    "    weight = pd.DataFrame(fit_t.toarray())\n",
    "\n",
    "    return weight,data[\"label\"].values\n",
    "\n",
    "word, x_total, y_total = vectorize(train)\n",
    "\n",
    "f1 = SelectKBest(chi2, k=130)\n",
    "f2 = f1.fit_transform(x_total, y_total)\n",
    "l = f1.get_support(indices=True).tolist()\n",
    "feature = {}\n",
    "for i, v in enumerate(l):\n",
    "    feature[word[v]] = i \n",
    "with codecs.open(\"selected_feature.txt\",\"w+\",\"utf-8\") as f:\n",
    "    for i in feature:\n",
    "        f.write(i+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Xgboost as our model ,and got accuracy of   0.88191542 on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                             review\n",
      "0   pos  movie gets respect sure lot memorable quotes l...\n",
      "1   pos  bizarre horror movie filled famous faces stole...\n",
      "2   pos  solid unremarkable film matthau einstein wonde...\n",
      "3   pos  strange feeling sit alone theater occupied par...\n",
      "4   pos  probably already know 5 additional episodes ne...\n",
      "  label                                             review\n",
      "0   pos  based actual story john boorman shows struggle...\n",
      "1   pos  gem film four production anticipated quality i...\n",
      "2   pos  really like show drama romance comedy rolled o...\n",
      "3   pos  best 3 experience disney themeparks certainly ...\n",
      "4   pos  korean movies seen three really stuck first ex...\n",
      "train_auc 0.9141816\n",
      "test_auc 0.88191542\n"
     ]
    }
   ],
   "source": [
    "ftre = set()\n",
    "with codecs.open('selected_feature.txt', 'r+', 'utf-8') as f:\n",
    "    for idx, line in enumerate(f.readlines()):\n",
    "        line = line.strip('\\n').split()\n",
    "        ftre.add(line[0])\n",
    "f.close()\n",
    "feature = {b: a for a, b in enumerate(ftre)}\n",
    "\n",
    "x_train, y_train = vectorize2(train,feature)\n",
    "x_test, y_test = vectorize2(test, feature)\n",
    "\n",
    "clf=XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0.1,\n",
    "    learning_rate=0.3,\n",
    "    max_delta_step=0,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=50,\n",
    "    nthread=-1,\n",
    "    objective='binary:logistic',\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0,\n",
    "    silent=True,\n",
    "    subsample=1)\n",
    "clf.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_metric=\"auc\",\n",
    "    eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "    verbose=False)\n",
    "\n",
    "evals_result = clf.evals_result()\n",
    "\n",
    "print \"train_auc\", sum(evals_result['validation_0']['auc'])/float(len(evals_result['validation_0']['auc']))\n",
    "print \"test_auc\",sum(evals_result['validation_1']['auc'])/float(len(evals_result['validation_0']['auc']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then I used deep learning method and still using TF-IDF vetorization  as features. The accuracy on test data is 0.839039981365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "\n",
    "def get_last_layer_units_and_activation(num_classes):\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    op_units, op_activation = get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    \n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(data ,learning_rate=1e-3, epochs=1000, batch_size=128, layers=2, units=64, \n",
    "                      dropout_rate=0.2):\n",
    "    \n",
    "    num_classes = 2\n",
    "    \n",
    "    # Get the data\n",
    "    \n",
    "    # Verify the validation labels\n",
    "    '''\n",
    "    unexpected_labels = [v for v in ttY if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the labels'\n",
    "                         ' in the validation set are in the same range as '\n",
    "                         'training labels.'.format(unexpected_labels=unexpected_labels))\n",
    "    '''\n",
    "    x_train, y_train, x_test,y_test=data\n",
    "    \n",
    "    # Create model instance\n",
    "    model = mlp_model(layers, units=units, dropout_rate=dropout_rate,\n",
    "                      input_shape=x_train.shape[1:], num_classes=num_classes)\n",
    "    \n",
    "    # Compile model with parameters\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "    \n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease on two consecutive tries, stop training\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "    \n",
    "    # Train and validate model\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test),\n",
    "                        verbose=2, batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    # Print results\n",
    "    history = history.history\n",
    "    val_acc = history['val_acc'][-1]\n",
    "    val_loss = history['val_loss'][-1]\n",
    "    print ('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=val_acc, loss=val_loss))\n",
    "    \n",
    "    # Save model\n",
    "    model.save('IMDB_mlp_model_' + str(val_acc) + '_' + str(loss) + '.h5')\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 0.5327 - acc: 0.7700 - val_loss: 0.3881 - val_acc: 0.8343\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.4088 - acc: 0.8158 - val_loss: 0.3678 - val_acc: 0.8383\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.4067 - acc: 0.8161 - val_loss: 0.3658 - val_acc: 0.8386\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.4032 - acc: 0.8160 - val_loss: 0.3651 - val_acc: 0.8393\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.4003 - acc: 0.8192 - val_loss: 0.3641 - val_acc: 0.8390\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.4007 - acc: 0.8173 - val_loss: 0.3642 - val_acc: 0.8390\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.3963 - acc: 0.8200 - val_loss: 0.3647 - val_acc: 0.8390\n",
      "Validation accuracy: 0.839039981365, loss: 0.36467838253\n"
     ]
    }
   ],
   "source": [
    "results = train_ngram_model((x_train, y_train, x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.The following part is using wordvetors (downlod glove) as our feature, and using LSTM as our model , The final accurcy is 0.85  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "loading complete!\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FP = 'train.csv'\n",
    "TEST_FP = 'test.csv'\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "#set gpu config\n",
    "# config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)\n",
    "# print 'Using 1 GPU and 4 cores of CPU'\n",
    "\n",
    "#read data\n",
    "print 'loading data...'\n",
    "train_data, test_data = pd.read_csv('train.csv'), pd.read_csv('test.csv')\n",
    "texts = train_data[\"review\"]\n",
    "labels = train_data[\"label\"]\n",
    "print 'loading complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text and turing them into sequences...\n",
      "process complete!\n",
      "pading sequences...\n",
      "padding complete!\n"
     ]
    }
   ],
   "source": [
    "print 'Tokenizing text and turing them into sequences...'\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "#sequences is a array of number[123, 456, 12 ...]\n",
    "print 'process complete!'\n",
    "\n",
    "#word index is the dictionary of sequences\n",
    "word_index = tokenizer.word_index\n",
    "print 'pading sequences...'\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print 'padding complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "#turn the labels into categotical\n",
    "# label = dict()\n",
    "# for i in range(len(labels)):  \n",
    "#     print labels[i]\n",
    "#     labels[i] = to_categorical(np.asarray(labels[i]))\n",
    "mapper={\"pos\":1,\"neg\":0}\n",
    "x=labels.map(mapper)\n",
    "#print x\n",
    "df=pd.DataFrame(data)\n",
    "df['label']=x\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "#print df[:10]\n",
    "data=df.iloc[:,:200]\n",
    "labels=df.iloc[:,200]\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pre-trained word embedding...\n",
      "reading complete!\n",
      "making embedding matrix..\n",
      "process complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "#read pre-trained word embedding and turn them into a dictionary call embedding_index\n",
    "print 'reading pre-trained word embedding...'\n",
    "embeddings_index = {}\n",
    "with open('./glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print 'reading complete!' \n",
    "#\n",
    "print 'making embedding matrix..'\n",
    "EMBEDDING_DIM = 300\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print 'process complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64    label\n",
      "0      1\n",
      "1      1\n",
      "2      0\n",
      "3      1\n",
      "4      1\n",
      "   label  label2\n",
      "0      1       0\n",
      "1      1       0\n",
      "2      0       1\n",
      "3      1       0\n",
      "4      1       0\n"
     ]
    }
   ],
   "source": [
    "mapper={0:1,1:0}\n",
    "tmp=labels['label'].map(mapper)\n",
    "#print tmp[:5],labels[:5]\n",
    "labels['label2']=tmp\n",
    "#print labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 70s 3ms/step - loss: 0.4462 - acc: 0.7928 - val_loss: 0.2967 - val_acc: 0.8740\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.946871\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 64s 3ms/step - loss: 0.2100 - acc: 0.9222 - val_loss: 0.2946 - val_acc: 0.8758\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.947932\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 65s 3ms/step - loss: 0.1291 - acc: 0.9569 - val_loss: 0.3698 - val_acc: 0.8690\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.943647\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18288c1fd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPool1D\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "x = Embedding(num_words, EMBEDDING_DIM)(inp)\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "mapper={0:1,1:0}\n",
    "tmp=labels['label'].map(mapper)\n",
    "labels['label2']=tmp\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "f\n",
    "\n",
    "yt = labels[:-num_validation_samples]\n",
    "yv = np.array(labels[-num_validation_samples:], dtype = float)\n",
    "\n",
    "ra = RocAucEvaluation(validation_data=(x_val, yv), interval = 1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=1, verbose=1)\n",
    "\n",
    "model.fit(x_train, yt,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks = [ra, early_stopping],\n",
    "          validation_data=(x_val, yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test_data['review']\n",
    "label=pd.DataFrame(test_data['label'])\n",
    "mapper={0:1,1:0}\n",
    "tmp=label['label'].map(mapper)\n",
    "label['label2']=tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tokenizer.texts_to_sequences(test)\n",
    "test2 = pad_sequences(test1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test_data['review']\n",
    "label=pd.DataFrame(test_data['label'])\n",
    "mapper={0:1,1:0}\n",
    "tmp=label['label'].map(mapper)\n",
    "label['label2']=tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "for i in range(len(pred)):\n",
    "    if pred[i][0]>0.5 :\n",
    "        tmp.append(1)\n",
    "    else:\n",
    "        tmp.append(0)\n",
    "label=test_data['label']\n",
    "mapper={'pos':1,'neg':0}\n",
    "label=label.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.81      0.84     12500\n",
      "    class 1       0.82      0.89      0.86     12500\n",
      "\n",
      "avg / total       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = list(label)\n",
    "y_pred = tmp\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
